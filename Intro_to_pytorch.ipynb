{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd() # it is a good idea to check and document which folder your notebook is saved in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists, arrays, and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the tutorial at\n",
    "https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "\n",
    "Please follow through that tutorial as well. \n",
    "I tell you slightly different things here. You will gain from doing both tutorials (but I recommend that you do mine first! ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2], [3, 4]] # this makes a Python list of lists: this is not a proper array\n",
    "data # to see what it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( data ) # type is a very useful Python function: \n",
    "# you should always know what type every variable is in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a list of data into a numpy array (I hope you already know how to do this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.array( data )\n",
    "np_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( np_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type does not give all useful information about a numpy array. Numpy arrays also have attributes, which give their shape and the data type of their elements.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should ALWAYS make sure you know the shape of the numpy arrays in your code, and you should also be careful to keep track of the data type of the individual elements. \n",
    "\n",
    "If numpy had been part of Python from the beginning, these attributes would be part of the type itself..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "The point of this worksheet is to introduce pytorch tensors. These are very similar to numpy arrays, but, as we shall see, you can do some additional things with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data = torch.from_numpy( np_data )\n",
    "torch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just created a pytorch tensor from a numpy array.  (There are many other ways to create tensors -- see the other tutorial for some examples.)\n",
    "\n",
    "We can use a torch tensor just like a numpy array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 17],\n",
       "        [ 3,  4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data[0,1] = 17\n",
    "torch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the numpy array from which we created the tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 17],\n",
       "       [ 3,  4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wow! What has happened?  The value in the numpy array has also changed.\n",
    "\n",
    "This means that the torch tensor and the numpy array share the same data: the torch tensor has some additional information and properties which we will now turn to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Heavily) adapted from the tutorial at https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "Please look at that tutorial also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up a single neuron with tensors. \n",
    "\n",
    "There will be three inputs `x=[1,2,3]` and three weights `w`\n",
    "and one output, which will go through a relu function. \n",
    "\n",
    "We will perform the forward pass and the backward pass, and compute the gradients of the output with respopect to the weights. \n",
    "\n",
    "The pre-activation of the neuron is the weighted sum of the inputs: \n",
    "\n",
    "$a = w_1 x_1 + w_2 x_2 + w_3 x_3$\n",
    "\n",
    "The activation, or output, of the neuron is\n",
    "\n",
    "$relu(a) = \\max( a, 0)$ \n",
    "\n",
    "We need to set all this up with tensors instead of arrays, for reasons which will become clear. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the weights. There is one weight for each input. We want to compute the gradient of the output wrt the weights and the inputs, and to do this we need to set the tensor attribute `requires_grad` to be `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6987, -0.0979,  0.6506], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn_like( x, requires_grad=True )\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0571, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sum( w * x )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0571, grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.max( a, torch.tensor(0.0))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the forward-pass, which is computing the output from the input. \n",
    "\n",
    "Now we compute the gradients. \n",
    "\n",
    "We could do this by hand: \n",
    "\n",
    "$\\frac{\\partial y}{\\partial w_1} = \\frac{\\partial a}{\\partial w_1}\\cdot \\frac{\\partial y}{\\partial a} = x_1 \\cdot 1 = x_1 $\n",
    "\n",
    "For these values of $w$ and $x$, $a > 0$, so $\\frac{\\partial y}{\\partial a}=1$. \n",
    "\n",
    "If $a < 0 $, $\\frac{\\partial y}{\\partial a} = 0$. \n",
    "\n",
    "But we don't need to do it by hand: we can use autodiff.  Because `requires_grad` was set to true for `w` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems to have happened - but a lot has happened. The computational graph for the calculation of y from x and w was retained, and the gradients of all tensors in the computational graph wrt y were calculated. \n",
    "\n",
    "We can look at them like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6987, -0.0979,  0.6506])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the gradient of $\\frac{\\partial y}{\\partial w} = x \\cdot \\frac{\\partial y}{\\partial a}$ and \n",
    "$\\frac{\\partial y}{\\partial x} = w \\cdot \\frac{\\partial y}{\\partial a}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about `a`, the pre-activation? Do we have the gradient $\\frac{\\partial y}{\\partial a}$ ?\n",
    "\n",
    "Let's see: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-d40920619642>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  a.grad\n"
     ]
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah - pytorch has assumed that we do not need the gradients for intermediate results, and it has disposed of it. We could have kept it...but really we were only looking out of curiosity. \n",
    "\n",
    "The leaf-tensors are the tensors at the start and the end of the computational graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to note\n",
    "\n",
    "1. Convince yourself that the gradients are correct\n",
    "\n",
    "2. Try running the calculation again. You will find that the gradients accumulate (that is, the new gradients are added to the existing gradients). \n",
    "This is a feature, not a bug!  It is often useful to accumulate gradients from many different calculations, before using them. \n",
    "\n",
    "You can reset the gradients to zero (well actually, you can delete them) by calling `x.grad = None` and `w.grad=None` before recomputing. \n",
    "\n",
    "3. Notice that we used special `torch` functions in the forward pass, instead of ordinary python or numpy functions.  This is because each `torch` forward function has another backward function that is used to compute is gradient, when working back through the computational graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x.shape`gives the shape of the tensor: the size of each of its dimensions. Very useful - you always need to know the shapes of the tensors you are using.  \n",
    "\n",
    "`x.dtype` gives the data type of the tensor elements (this will always be a number-type). float32 is the default, and this is the one that gives fast arithmetic. \n",
    "\n",
    "`x.device` is only relevant if you have a gpu: then you will keep needing to transfer tensors between the cpu memory and the gpu memory. If you are just using a cpu, life is much simpler and you can ignore this attribute. \n",
    "\n",
    "`x.requires_grad` if this is set to true, then the computational graph will be kept for calculations involving this tensor, so that gradients can be calculated. \n",
    "\n",
    "`x.grad`is the gradient that has been calculated for the tensor x after y.backward() has been called, where y is a leaf (a final single-number tensor result) of the calculation. \n",
    "    Note that y needs to be a single number, because otherwise the derivatives of x need to be calculated with respect to each element of y: don't go there. All our gradient calculations will be for single-valued results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = x.size()\n",
    "sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( sz )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x.shape` is an alias for `x.Size()` -- you can use either. \n",
    "\n",
    "There are many ways to create tensors: another is to use the functions `ones_like` or `zeros_like` or `randn_like` which we use here. `torch.ones_like(x)` creates a new tensor of the same size as `x`, but filled with 1.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do the pytorch blitz tutorial\n",
    "\n",
    "at  https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
