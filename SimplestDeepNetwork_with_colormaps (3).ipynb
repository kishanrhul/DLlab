{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The simplest deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to investigate how even the simplest possible \"deep network\" can lead to instabilities (or at least difficulties) in learning, and how a small amount of regularisation can improve this. \n",
    "\n",
    "This exercise shows how regularisation is desirable when there is a large set of equally good solutions, but some of these \"equally good\" solutions have very large and very small weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our standard imports, and the command to make plots appear within the notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make some data. \n",
    "\n",
    "$y = x + \\mathrm{noise}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = 10 * ( np.random.random([1000]) - 0.5)\n",
    "yy = xx + 0.5 * np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape, yy.shape # always check what you have produced, and then plot it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xx, yy,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent to estimate $w$ in $\\hat{y}=wx$, and squared loss.\n",
    "\n",
    "The loss (for a single data item with $x$, prediction $\\hat{y}$, and true value $y$) is: $J = \\frac{1}{2} (\\hat{y} - y)^2$\n",
    "\n",
    "The prediction is:  $\\hat{y} = w x $\n",
    "\n",
    "We put in the factor of $\\frac{1}{2}$ in the definition of $J$ for convenience, so we don't have factors of 2 in all the derivatives. \n",
    "\n",
    "So $$\\frac{\\partial J}{\\partial w} = (\\hat{y} - y) \\frac{\\partial \\hat{y}}{ \\partial w}  = (\\hat{y} - y) x$$\n",
    "\n",
    "Gradient descent for $w$ is implemented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with w set to be the exact correct value ! \n",
    "w = 1.0 \n",
    "\n",
    "# ww will correct values of w after they are updated by each data point\n",
    "ww = [w]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for x,y in zip(xx,yy):\n",
    "    yhat = w * x \n",
    "    grad = (yhat - y) * x \n",
    "    w = w - learning_rate * grad\n",
    "    ww.append(w) # collect each new value of w\n",
    "    \n",
    "# Now plot the values of w as they are adjusted during learning\n",
    "\n",
    "# the values of w should stay close to the optimal value (1.0), but they will vary around it because of \n",
    "# stochastic gradient descent updates\n",
    "\n",
    "# but provided that the learning rate is set low enough, the system is stable. For what value of the \n",
    "# learning rate does this become unstable? \n",
    "\n",
    "plt.plot(ww)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a deep model for the same problem and data. \n",
    "\n",
    "Have two weights, $w_1$, and $w_2$, and set\n",
    "\n",
    "$$ \\hat{y} = w_1 w_2 x $$\n",
    "\n",
    "Of course this is not very deep (only two layers!) and it is a very silly model because we just multiply the weights\n",
    "to produce $w = w_1 w_2$ , which is the same model as before. \n",
    "\n",
    "Nevertheless, this is perhaps the simplest possible \"deep\" model. As we will shortly see, we have introduced instability and we will need regularisation ! \n",
    "\n",
    "As before, for a single data item with $x$, prediction $\\hat{y}$, and true value $y$, the loss is: \n",
    "\n",
    "$$J = \\frac{1}{2} (\\hat{y} - y)^2$$\n",
    "\n",
    "In this model the prediction is:  \n",
    "\n",
    "$$\\hat{y} = w_1 w_2 x $$\n",
    "\n",
    "so \n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial w_1} = w_2 x $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial w_2} = w_1 x $$\n",
    "\n",
    "If either of the weights gets large, one of the gradients will become large, and this may cause instability. \n",
    "\n",
    "What weights are optimal? Well we only need $w_1 w_2 = 1$, so any values of $w_1$ and $w_2$ are acceptable so long as\n",
    "\n",
    "$$ w_1 = \\frac{1}{w_2}$$\n",
    "\n",
    "So one weight can become large as long as the other weight becomes small...\n",
    "\n",
    "\n",
    "Gradient descent for this \"deep\" model is implemented below\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_J(x,y,w1,w2):\n",
    "    \"\"\"\n",
    "    x, y are ndarrays of the same length\n",
    "    w1 and w2 are weights of 2 layers of network\n",
    "    \"\"\"\n",
    "    yhat  = w1 * w2 * x\n",
    "    errs = (yhat - y)**2\n",
    "    J = np.mean( errs )\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_values = np.linspace(0.01,4,100)\n",
    "w2_values = np.linspace(0.01,4,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_grid = np.zeros([w1_values.size,w2_values.size])\n",
    "\n",
    "for w1_index in range(0,w1_values.size):\n",
    "    for w2_index in range(0,w2_values.size):\n",
    "        J_grid[w1_index,w2_index] = calculate_J(xx, yy, w1_values[w1_index], w2_values[w2_index] )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(1/J_grid.transpose(), origin='lower',extent=[0,1,0,2])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start off with w1 and w2 different, but with w1 * w2 == 1.0 as before\n",
    "w1 = 0.7\n",
    "w2 = 1.0/0.7\n",
    "\n",
    "# now we will collect both weights as learning proceeds\n",
    "ww1 = [w1]\n",
    "ww2 = [w2]\n",
    "\n",
    "# try learning rates higher and lower than this value\n",
    "learning_rate = 0.01\n",
    "\n",
    "for x,y in zip(xx,yy):\n",
    "    yhat = w1 * w2 * x \n",
    "    grad = (yhat - y) * x \n",
    "    w1 = w1 - learning_rate * grad * w2\n",
    "    w2 = w2 - learning_rate * grad * w1\n",
    "    \n",
    "    ww1.append(w1) # collect each new value of w1\n",
    "    ww2.append(w2) # collect each new value of w2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000 # this is for you to fill in: find out by trial and error when the estimate goes wild, and plot just up to \n",
    "        # an integer you might call 'limit', to see what happened just before the estimated w heads off to infinity\n",
    "plt.plot(ww1[:limit],ww2[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also plot the estimated values of w up until they go wild.  \n",
    "# The correct value of w is always 1: can you see how the errors get slowly larger until there is a catastrophic growth?\n",
    "\n",
    "plt.plot( np.array(ww1[:limit]) * np.array(ww2[:limit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-help challenge: \n",
    "\n",
    "Find out (by googling or by consulting the matplotlib documentation) how to plot a contour plot of $(w_1 w_2 - 1)^2$, for $w_1, w_2 > 0$.  (This is proportional to the expected loss $J$ as a function of $w_1, w_2$ .\n",
    "\n",
    "It is useful to be able to plot contour plots, and this graph will give you insight as to why the instability occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing regularisation\n",
    "\n",
    "Now add a regularising term to $J$. \n",
    "\n",
    "$$ J = = \\frac{1}{2} \\left( (\\hat{y} - y)^2 + \\lambda (w_1^2 + w_2^2) \\right)$$\n",
    "\n",
    "This punishes large values of $w_1$ or $w_2$. \n",
    "\n",
    "$$ \\frac{ \\partial J}{\\partial w_1} = (\\hat{y} - y) w_2 x + \\lambda w_1 $$\n",
    "\n",
    "Exercise: write down $$ \\frac{ \\partial J}{\\partial w_1} $$\n",
    "\n",
    "The gradient descent code is written below. \n",
    "\n",
    "Try out different values of the regulariser : what is the effect of different values? \n",
    "\n",
    "What is the effect when the regulariser is too large? \n",
    "\n",
    "How small is too small? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulariser = 0.01 # quite big ! Try both larger and smaller values\n",
    "\n",
    "# we start off with w1 and w2 different, but with w1 * w2 == 1.0 as before\n",
    "w1 = 0.7\n",
    "w2 = 1.0/0.7\n",
    "\n",
    "# now we will collect both weights as learning proceeds\n",
    "ww1 = [w1]\n",
    "ww2 = [w2]\n",
    "\n",
    "yhats = []\n",
    "\n",
    "# try learning rates higher and lower than this value\n",
    "learning_rate = 0.019\n",
    "\n",
    "for x,y in zip(xx,yy):\n",
    "    yhat = w1 * w2 * x \n",
    "    grad = (yhat - y) * x \n",
    "    w1 = w1 - learning_rate * grad * w2 - regulariser * w1\n",
    "    w2 = w2 - learning_rate * grad * w1 - regulariser * w2\n",
    "    \n",
    "    yhats.append(yhat)\n",
    "    ww1.append(w1) # collect each new value of w1\n",
    "    ww2.append(w2) # collect each new value of w2\n",
    "    \n",
    "# now visualise ww1, ww2 as before to see if there has been instability\n",
    "\n",
    "# also plot yy versus yhats : what happens when the regulariser is too big? \n",
    "# What is the compromise involved in introducing regularisation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
