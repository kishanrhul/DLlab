{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Dataset\n",
    "\n",
    "#  Aims of this practical session\n",
    "\n",
    "* To define and use different deep models using the Keras Models API, which is more general than the Sequential API we have used so far\n",
    "\n",
    "* To examine the effect of batch normalisation and skip-forward connections on learning with deep networks. \n",
    "\n",
    "* To examine the effect of different amounts of regularisation on the same architecture, and to plot a graph of training and validation error as a function of the amount of regularisation\n",
    "\n",
    "* To explore the MNIST dataset and plot learning curves to estimate error rates as a function of training set size\n",
    "\n",
    "* To do your own exploration of applying different architectures to the MNIST data (and possibly also to the CIFAR data during the week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm  # colormaps\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # this includes tensor functions that we can use in backwards pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch datasets\n",
    "\n",
    "Several well known datasets are provided with pytorch, and can be imported and easily used.\n",
    "\n",
    "The MNIST dataset is a famous and has been used as a testbed of machine learning algorithms for more than 25 years. \n",
    "\n",
    "There is an interesting dataset called fashion_MNIST which is a plug-in replacement for the MNIST dataset, but which may have very different properties (it is grey-scale images). (It is possible that you may need to download this from the web if you want to use it. ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.0], std=[1.0,]) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell below downloads the MNIST dataset, and extracts it into the training set and the test set. The transforms above are used to alter the format to float32 and normalise the input values to small numbers\n",
    "\n",
    "Downloading may take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e013f64fda481a8a0a4f6db52899ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6152d1514fff4047be9b269b308a4d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80af41fecfc43b89989567d8e8e1feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6453a1b9d488425a8048b5a8d0df066b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cjchw\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1de4448f9a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( mnist_trainset[0][0].view(28,28).numpy() )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have normalised the values in the image to lie between 0 and 1.  However, I am baffled! According to the Pytorch documentation, the Normalize transform with the arguments I have given to it should normalise each image to have pixel values with a mean of 0 and a standard deviation of 1.  I am baffled....can anyone resolve this?  \n",
    "\n",
    "I do not know whether a normalisation between 0 and 1, or a normalisation with mean 0 and std 1 would be better. One might try both. It is possible that the normalisation between 0 and 1 is superior because 0 has the meaning \"blank space\" and 1 has the meaning \"dark ink\" for every image. \n",
    "\n",
    "The exact form of normalisation may not be important, but it is important to do some normalisation to make the inputs of a defined size and -- most importantly -- that the inputs should be of roughly similar typical sizes. \n",
    "\n",
    "The reason for ensuring that inputs are of similar typical magnitudes is that the gradient of an input weight is proportional to the input: if we increase the typical values of one input by a factor of 10, the weight must be reduced by a factor of 10, but its gradients will increase by a factor of 10... This makes optimisation more difficult.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a smaller set of data to train with at first, so that you can do more experiments more quickly. Let's take the first 4000 training examples only: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset_small =  [ mnist_trainset[i] for i in range(0,4000) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( mnist_trainset ) # unfortunately this type information is not very informative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( mnist_trainset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! so this training set consists of a list of 60,000 pairs, each of which is an image and a digit\n",
    "\n",
    "The first thing to do with any data is to look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0 = mnist_trainset[0][0]\n",
    "\n",
    "im0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display this as a heatmap in numpy we need to reshape from (1,28,28) to (28,28), and convert to a numpy array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de45cd3370>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( im0.reshape(28,28).numpy()) # to pass it to matplotlib, we need to convert it to a 2 dimensional numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at more of the digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will plot a grid of images with their titles\n",
    "\n",
    "def plot_images(images_to_plot, titles=None, ncols=6, thefigsize=(18,18)):\n",
    "    \n",
    "    n_images = images_to_plot.shape[0]\n",
    "    \n",
    "    nrows = np.ceil(n_images/ncols).astype(int)\n",
    "    \n",
    "    fig,ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=thefigsize)\n",
    "    ax = ax.flatten() # we can number the axes from 0 upwards with a single integer\n",
    "\n",
    "    for i in range(n_images):\n",
    "        ax[i].imshow( images_to_plot[i,0,:,:].reshape(28,28), cmap=cm.Greys ) \n",
    "            # cmap=cm.Greys plots in Grey scale so the image looks as if it were written\n",
    "        ax[i].axis('off')  \n",
    "        if titles is not None:\n",
    "            ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_plot = np.array( [ mnist_trainset_small[i][0].numpy() for i in range(0,36)])\n",
    "titles = [ mnist_trainset_small[i][1] for i in range(0,36) ] \n",
    "plot_images( images_to_plot, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying these isn't easy: look how different the 4s are from each other, and the 5s. \n",
    "\n",
    "Take a moment to look at the images carefully: what do you think are the difficulties of classifying these with a neural net?  How large are the features that you would need to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models for MNIST classification\n",
    "\n",
    "It is best not to use the entire MNIST dataset as a training set because you will be training for the whole of the practical class. You will learn much more by training repeatedly on smaller subsets, and examining the effect of using different models and parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to construct a (simple) neural network is to use the high-level constructor nn.Sequential, which simply chains a number of layers together. (This constructor hides some details - but not many details, and we will look at those later.) \n",
    "\n",
    "This constructs tensors containing initialised weights, and it wraps them in an object that can compute the forward (and backward) pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = nn.Sequential( \n",
    "        nn.Conv2d(1,6,3), # takes one input channel (greyscale), gives 6 output channes, each from a 3x3 convolutional neuron\n",
    "        nn.ReLU()) \n",
    "      #  nn.Conv2d(6,6,3), # takes 26 x 26 to 24 x 24\n",
    "       # nn.ReLU(), \n",
    "      #  nn.MaxPool2d(2,2) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( model1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 26, 26])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model1( mnist_trainset[0][0].view(1,1,28,28) )\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = [] # reclaim the space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally it is better practice to construct a NN by subclassing the pytorch class nn.Module, so that we have all the code necessary for initialising the neural net and doing the forward pass in one place.  \n",
    "\n",
    "(A very common and easy mistake in working with a NN is to forget to re-initialise it, and then train it repeatedly under different conditions without resetting the weights to random initialised values: this produces bad experimental results, which may take a while to identify...therefore it is MUCH SAFER to automatically provide yourself with a class for your NN so that you can automatically initialise new instances of it...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our simplest neural net - it is just going to be a linear layer and then softmax\n",
    "class NN1( nn.Module ):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(NN1, self).__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784,10))  # a linear layer (a matrix, plus biases) with 784 inputs and 10 outputs\n",
    "             # if we are going to make probabilistic categorical predictions, it is more flexible to allow unnormalised \n",
    "             # outputs, because we can use  different functions for the predictions and and in calculating the loss, as \n",
    "             # we shall soon see\n",
    "    \n",
    "    \n",
    "    def forward( self, x ): # computes the forward pass ... this one is particularly simple\n",
    "        x = self.layers( x )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = NN1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0796, -0.1956,  0.0471, -0.0576, -0.1083,  0.3235,  0.0865, -0.1110,\n",
       "         -0.0092, -0.1041]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = nn1.forward( mnist_trainset[0][0])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output consists of unbounded positive and negative values - it needs to be converted to probabilities to make a probabilistic predictions of each of the 10 classes. \n",
    "\n",
    "The standard and most commonly used method of mapping arbitrary numeric values to probabilities is softmax \n",
    "\n",
    "Let's see the predictions using softmax: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0933, 0.0831, 0.1059, 0.0954, 0.0907, 0.1397, 0.1102, 0.0904, 0.1001,\n",
       "         0.0911]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_probs = F.softmax( tmp, dim=1)\n",
    "tmp_probs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might hope, these predictions are random values of approximately 0.1 : this is what we should expect, because the network has been initialised with random weights, and it has not yet learned to make good predictions. \n",
    "\n",
    "Now, the true class of the 0th item in the trainset is 5. \n",
    "\n",
    "Let us find the log-loss for this prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09333082, 0.08310757, 0.10593501, 0.0954067 , 0.09069076,\n",
       "        0.13966897, 0.11019802, 0.09044626, 0.10014161, 0.09107427]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to examine the values of tensors with gradients attached, we must first detach the gradient, and then \n",
    "# convert to numpy, thus: \n",
    "tmp_probs_n = tmp_probs.detach().numpy()\n",
    "tmp_probs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13966897"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_probs_n[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9684801"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log( tmp_probs_n[0,5] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last: this is the log-loss, which is the negative log of the predicted probability of the true class (in this case 5) \n",
    "\n",
    "A typical predicted probability will be near to 0.1, with a loss that is near to -np.log(0.1) which is -2.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to construct the loss function, which will produce a single loss for a batch of examples. This is the final leaf of the computational graph, from which we back-propagate to find the gradients. \n",
    "\n",
    "There is a single torch function that will take unnormalised outputs, and combine softmax and log-loss efficiently: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() \n",
    "# I am unsure why the loss function has to be created like this; the documentation does not enlighten me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9685, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function( tmp, torch.LongTensor([5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent - this loss should be the same as the one we computed directly \n",
    "\n",
    "We now need to check that we can compute the losses correctly for a batch of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is a useful utility class provided by pytorch for extracting minibatches from \n",
    "# a large dataset. \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( mnist_trainset_small, batch_size=4, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader( mnist_testset, batch_size=4, shuffle=True)\n",
    "\n",
    "# if you want to change the batch size, you need to define a new dataloader: \n",
    "# the dataloader is a lightweight class that just provides iteration over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we construct a (temporary) iterator from trainloader, and we get the first minibatch\n",
    "tmpiter = iter(trainloader)\n",
    "\n",
    "images, labels = tmpiter.next() # the minibatch is a list of [ images, labels ]\n",
    "\n",
    "# let's check what form the images and labels come in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # the first index is the data item, then it is 1x28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 7, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0103, -0.1198, -0.2532, -0.1224,  0.0168,  0.2111, -0.0562, -0.1871,\n",
       "          0.0138,  0.0679],\n",
       "        [ 0.3531,  0.0146, -0.1720, -0.0741, -0.1710,  0.2341,  0.0029, -0.1459,\n",
       "         -0.1867, -0.3274],\n",
       "        [-0.2412,  0.3560, -0.1234, -0.3206, -0.2233,  0.4257, -0.1279, -0.2392,\n",
       "         -0.0168, -0.5086],\n",
       "        [-0.2341, -0.0715, -0.2659, -0.0658, -0.3172,  0.3274, -0.1531, -0.2684,\n",
       "          0.1624, -0.1116]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = nn1.forward( images )\n",
    "tmp  # note that these are 4 sets of probabilistic predictions, but unnormalised by softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2998, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we compute the loss: the loss function computes all the individual losses, and averages them\n",
    "loss_function(tmp, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the .backward() method on the output of the loss_function, we can back-propagate and compute the gradients of the loss wrt all the weights. \n",
    "\n",
    "To optimise the weights, it is convenient to use another class, that takes the neural net parameters, and updates them using the gradients. Many optimisation methods - such as RMSProp and ADAM - require additional data structures (moving averages of gradients and squared gradients). \n",
    "\n",
    "The .parameters() method gives the parameters of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001DE444D60B0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [ p for p in nn1.parameters() ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0239, -0.0033,  0.0074,  ...,  0.0049,  0.0271,  0.0294],\n",
       "         [ 0.0099,  0.0341,  0.0344,  ..., -0.0344, -0.0177,  0.0239],\n",
       "         [-0.0348,  0.0242, -0.0015,  ...,  0.0213, -0.0100,  0.0115],\n",
       "         ...,\n",
       "         [-0.0186, -0.0079, -0.0065,  ...,  0.0109,  0.0221,  0.0038],\n",
       "         [-0.0075, -0.0145, -0.0085,  ..., -0.0243, -0.0247,  0.0238],\n",
       "         [-0.0065,  0.0272,  0.0250,  ...,  0.0296,  0.0097, -0.0244]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.1895e-02,  2.7314e-02,  2.2021e-02, -1.2930e-05,  2.8325e-02,\n",
       "         -2.5938e-02, -3.1787e-02,  3.3480e-02, -2.5981e-02, -5.7703e-03],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([10, 784]), torch.Size([10])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ p.shape for p in params ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "optimizer1 = optim.RMSprop(nn1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to construct a learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.922\n",
      "Epoch 2 loss: 1.570\n",
      "Epoch 3 loss: 1.352\n",
      "Epoch 4 loss: 1.187\n",
      "Epoch 5 loss: 1.056\n",
      "Epoch 6 loss: 0.952\n",
      "Epoch 7 loss: 0.867\n",
      "Epoch 8 loss: 0.799\n",
      "Epoch 9 loss: 0.742\n",
      "Epoch 10 loss: 0.695\n",
      "Epoch 11 loss: 0.655\n",
      "Epoch 12 loss: 0.621\n",
      "Epoch 13 loss: 0.592\n",
      "Epoch 14 loss: 0.566\n",
      "Epoch 15 loss: 0.544\n",
      "Epoch 16 loss: 0.524\n",
      "Epoch 17 loss: 0.507\n",
      "Epoch 18 loss: 0.491\n",
      "Epoch 19 loss: 0.476\n",
      "Epoch 20 loss: 0.463\n",
      "Epoch 21 loss: 0.452\n",
      "Epoch 22 loss: 0.441\n",
      "Epoch 23 loss: 0.431\n",
      "Epoch 24 loss: 0.422\n",
      "Epoch 25 loss: 0.413\n",
      "Epoch 26 loss: 0.406\n",
      "Epoch 27 loss: 0.398\n",
      "Epoch 28 loss: 0.392\n",
      "Epoch 29 loss: 0.385\n",
      "Epoch 30 loss: 0.379\n",
      "Epoch 31 loss: 0.373\n",
      "Epoch 32 loss: 0.368\n",
      "Epoch 33 loss: 0.363\n",
      "Epoch 34 loss: 0.358\n",
      "Epoch 35 loss: 0.354\n",
      "Epoch 36 loss: 0.349\n",
      "Epoch 37 loss: 0.345\n",
      "Epoch 38 loss: 0.341\n",
      "Epoch 39 loss: 0.337\n",
      "Epoch 40 loss: 0.334\n",
      "Epoch 41 loss: 0.330\n",
      "Epoch 42 loss: 0.327\n",
      "Epoch 43 loss: 0.324\n",
      "Epoch 44 loss: 0.321\n",
      "Epoch 45 loss: 0.317\n",
      "Epoch 46 loss: 0.315\n",
      "Epoch 47 loss: 0.312\n",
      "Epoch 48 loss: 0.309\n",
      "Epoch 49 loss: 0.306\n",
      "Epoch 50 loss: 0.304\n",
      "Epoch 51 loss: 0.301\n",
      "Epoch 52 loss: 0.299\n",
      "Epoch 53 loss: 0.297\n",
      "Epoch 54 loss: 0.294\n",
      "Epoch 55 loss: 0.292\n",
      "Epoch 56 loss: 0.290\n",
      "Epoch 57 loss: 0.288\n",
      "Epoch 58 loss: 0.286\n",
      "Epoch 59 loss: 0.284\n",
      "Epoch 60 loss: 0.282\n",
      "Epoch 61 loss: 0.280\n",
      "Epoch 62 loss: 0.278\n",
      "Epoch 63 loss: 0.277\n",
      "Epoch 64 loss: 0.275\n",
      "Epoch 65 loss: 0.273\n",
      "Epoch 66 loss: 0.271\n",
      "Epoch 67 loss: 0.270\n",
      "Epoch 68 loss: 0.268\n",
      "Epoch 69 loss: 0.267\n",
      "Epoch 70 loss: 0.265\n",
      "Epoch 71 loss: 0.264\n",
      "Epoch 72 loss: 0.262\n",
      "Epoch 73 loss: 0.261\n",
      "Epoch 74 loss: 0.259\n",
      "Epoch 75 loss: 0.258\n",
      "Epoch 76 loss: 0.256\n",
      "Epoch 77 loss: 0.255\n",
      "Epoch 78 loss: 0.254\n",
      "Epoch 79 loss: 0.252\n",
      "Epoch 80 loss: 0.251\n",
      "Epoch 81 loss: 0.250\n",
      "Epoch 82 loss: 0.249\n",
      "Epoch 83 loss: 0.247\n",
      "Epoch 84 loss: 0.246\n",
      "Epoch 85 loss: 0.245\n",
      "Epoch 86 loss: 0.244\n",
      "Epoch 87 loss: 0.243\n",
      "Epoch 88 loss: 0.242\n",
      "Epoch 89 loss: 0.240\n",
      "Epoch 90 loss: 0.239\n",
      "Epoch 91 loss: 0.238\n",
      "Epoch 92 loss: 0.237\n",
      "Epoch 93 loss: 0.236\n",
      "Epoch 94 loss: 0.235\n",
      "Epoch 95 loss: 0.234\n",
      "Epoch 96 loss: 0.233\n",
      "Epoch 97 loss: 0.232\n",
      "Epoch 98 loss: 0.231\n",
      "Epoch 99 loss: 0.230\n",
      "Epoch 100 loss: 0.229\n",
      "Epoch 101 loss: 0.228\n",
      "Epoch 102 loss: 0.227\n",
      "Epoch 103 loss: 0.226\n",
      "Epoch 104 loss: 0.225\n",
      "Epoch 105 loss: 0.225\n",
      "Epoch 106 loss: 0.224\n",
      "Epoch 107 loss: 0.223\n",
      "Epoch 108 loss: 0.222\n",
      "Epoch 109 loss: 0.221\n",
      "Epoch 110 loss: 0.220\n",
      "Epoch 111 loss: 0.219\n",
      "Epoch 112 loss: 0.219\n",
      "Epoch 113 loss: 0.218\n",
      "Epoch 114 loss: 0.217\n",
      "Epoch 115 loss: 0.216\n",
      "Epoch 116 loss: 0.215\n",
      "Epoch 117 loss: 0.214\n",
      "Epoch 118 loss: 0.214\n",
      "Epoch 119 loss: 0.213\n",
      "Epoch 120 loss: 0.212\n",
      "Epoch 121 loss: 0.211\n",
      "Epoch 122 loss: 0.211\n",
      "Epoch 123 loss: 0.210\n",
      "Epoch 124 loss: 0.209\n",
      "Epoch 125 loss: 0.208\n",
      "Epoch 126 loss: 0.208\n",
      "Epoch 127 loss: 0.207\n",
      "Epoch 128 loss: 0.206\n",
      "Epoch 129 loss: 0.206\n",
      "Epoch 130 loss: 0.205\n",
      "Epoch 131 loss: 0.204\n",
      "Epoch 132 loss: 0.203\n",
      "Epoch 133 loss: 0.203\n",
      "Epoch 134 loss: 0.202\n",
      "Epoch 135 loss: 0.201\n",
      "Epoch 136 loss: 0.201\n",
      "Epoch 137 loss: 0.200\n",
      "Epoch 138 loss: 0.200\n",
      "Epoch 139 loss: 0.199\n",
      "Epoch 140 loss: 0.198\n",
      "Epoch 141 loss: 0.198\n",
      "Epoch 142 loss: 0.197\n",
      "Epoch 143 loss: 0.196\n",
      "Epoch 144 loss: 0.196\n",
      "Epoch 145 loss: 0.195\n",
      "Epoch 146 loss: 0.195\n",
      "Epoch 147 loss: 0.194\n",
      "Epoch 148 loss: 0.193\n",
      "Epoch 149 loss: 0.193\n",
      "Epoch 150 loss: 0.192\n",
      "Epoch 151 loss: 0.192\n",
      "Epoch 152 loss: 0.191\n",
      "Epoch 153 loss: 0.190\n",
      "Epoch 154 loss: 0.190\n",
      "Epoch 155 loss: 0.189\n",
      "Epoch 156 loss: 0.189\n",
      "Epoch 157 loss: 0.188\n",
      "Epoch 158 loss: 0.188\n",
      "Epoch 159 loss: 0.187\n",
      "Epoch 160 loss: 0.187\n",
      "Epoch 161 loss: 0.186\n",
      "Epoch 162 loss: 0.186\n",
      "Epoch 163 loss: 0.185\n",
      "Epoch 164 loss: 0.184\n",
      "Epoch 165 loss: 0.184\n",
      "Epoch 166 loss: 0.183\n",
      "Epoch 167 loss: 0.183\n",
      "Epoch 168 loss: 0.182\n",
      "Epoch 169 loss: 0.182\n",
      "Epoch 170 loss: 0.181\n",
      "Epoch 171 loss: 0.181\n",
      "Epoch 172 loss: 0.180\n",
      "Epoch 173 loss: 0.180\n",
      "Epoch 174 loss: 0.179\n",
      "Epoch 175 loss: 0.179\n",
      "Epoch 176 loss: 0.178\n",
      "Epoch 177 loss: 0.178\n",
      "Epoch 178 loss: 0.177\n",
      "Epoch 179 loss: 0.177\n",
      "Epoch 180 loss: 0.176\n",
      "Epoch 181 loss: 0.176\n",
      "Epoch 182 loss: 0.175\n",
      "Epoch 183 loss: 0.175\n",
      "Epoch 184 loss: 0.175\n",
      "Epoch 185 loss: 0.174\n",
      "Epoch 186 loss: 0.174\n",
      "Epoch 187 loss: 0.173\n",
      "Epoch 188 loss: 0.173\n",
      "Epoch 189 loss: 0.172\n",
      "Epoch 190 loss: 0.172\n",
      "Epoch 191 loss: 0.171\n",
      "Epoch 192 loss: 0.171\n",
      "Epoch 193 loss: 0.170\n",
      "Epoch 194 loss: 0.170\n",
      "Epoch 195 loss: 0.170\n",
      "Epoch 196 loss: 0.169\n",
      "Epoch 197 loss: 0.169\n",
      "Epoch 198 loss: 0.168\n",
      "Epoch 199 loss: 0.168\n",
      "Epoch 200 loss: 0.167\n"
     ]
    }
   ],
   "source": [
    "# notice that we create all the ingredients of the process again within this cell, \n",
    "# so that we train a newly initialised neural net with a newly initialised optimizer\n",
    "# containing the parameters of that neural net ....\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( mnist_trainset_small, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader( mnist_testset, batch_size=32, shuffle=True)\n",
    "\n",
    "nn1 = NN1()\n",
    "optimizer1 = optim.RMSprop( nn1.parameters(), lr=0.0001 )\n",
    "\n",
    "for epoch in range(200): # number of times to loop over the dataset\n",
    "    current_loss = 0.0 \n",
    "    n_mini_batches = 0\n",
    "    \n",
    "    for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "        images, labels = mini_batch\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        # all the parameters that are being updated are in the optimizer, \n",
    "        # so if we zero the gradients of all the tensors in the optimizer, \n",
    "        # that is the safest way to zero all the gradients\n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        outputs = nn1(images)\n",
    "        loss = loss_function ( outputs, labels )\n",
    "        loss.backward() # does the backward pass and computes all gradients\n",
    "        optimizer1.step() # does one optimisation step\n",
    "        \n",
    "        n_mini_batches += 1 \n",
    "        current_loss += loss.item() # remember that the loss is a zero-order tensor\n",
    "        # so that to extract its value, we use .item(), as we cannot index as there are no dimensions\n",
    "        \n",
    "     \n",
    "    print('Epoch %d loss: %.3f' %(epoch+1, current_loss / n_mini_batches ))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Validation accuracy and loss\n",
    "\n",
    "It's a good idea to turn off gradient computations when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 90 %\n"
     ]
    }
   ],
   "source": [
    "# this code calculates the error rate on the validation set\n",
    "correct = 0 \n",
    "total = 0 \n",
    "with torch.no_grad():  # we do not neet to compute the gradients when making predictions on the validation set\n",
    "    for data in testloader: \n",
    "        images, labels = data\n",
    "        outputs = nn1(images)\n",
    "        _, predicted = torch.max( outputs, dim=1)\n",
    "        total += labels.size(0) # the number of labels, which is just the size of the batch \n",
    "        correct += (predicted == labels).sum().item() # once again, note that the sum is a zero-dimensional tensor, \n",
    "                                                      # so we must access its value using .item()\n",
    "print( \"Accuracy on test set: %d %%\" %(100 * correct/total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do\n",
    "\n",
    "1. modify the learning loop to print out the validation accuracy after each epoch\n",
    "\n",
    "2. modify the learning loop to print both the validation accuracy and the loss after each epoch\n",
    "\n",
    "3. create multi-layer networks using any architecture that you want...and investigate their behaviour on this dataset\n",
    "   (You may find it faster and more interesting to train with the small training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining model parameters\n",
    "\n",
    "We can extract, examine, save and reload, and print the parameters of the model using the state_dict which\n",
    "is a dictionary of convenient names mapping to the model's parameter tensors:  \n",
    "\n",
    "\n",
    "Remember that we need to use .detach().numpy() to detach the value of a tensor from its gradients, and then create a numpy view of the values, before we can plot histograms of weights, etc and perform other analyses in numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state dict:\n",
      "layers.1.weight \t torch.Size([10, 784])\n",
      "layers.1.bias \t torch.Size([10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( \"Model's state dict:\")\n",
    "for param_tensor in nn1.state_dict():\n",
    "    print( param_tensor, \"\\t\", nn1.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the optimizer. \n",
    "\n",
    "Note that an RMSprop optimizer has non-trivial state, and the first element of the state is (annoyingly) a dict that contains a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state dict: \n",
      "state \t {0: {'step': 25000, 'square_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, 1: {'step': 25000, 'square_avg': tensor([4.7917e-05, 1.4140e-04, 2.5350e-04, 2.7831e-04, 2.1680e-04, 3.3515e-04,\n",
      "        9.6171e-05, 2.5513e-04, 3.2924e-04, 3.4000e-04])}}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-08, 'centered': False, 'weight_decay': 0, 'params': [0, 1]}]\n"
     ]
    }
   ],
   "source": [
    "print( \"Optimizer's state dict: \")\n",
    "for var_name in optimizer1.state_dict(): \n",
    "    print( var_name, \"\\t\", optimizer1.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading models\n",
    "\n",
    "We will often want to save a net that we have trained, and reload it. Pytorch provides convenient methods of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a path\n",
    "PATH = \"saved_state_dict.pt\" # pt suffice for checkpoint\n",
    "\n",
    "# Save the model\n",
    "torch.save( nn1.state_dict(), PATH)\n",
    "\n",
    "# Reload the model into a new neural network object\n",
    "nn1_reloaded = NN1()\n",
    "nn1_reloaded.load_state_dict( torch.load( PATH ))\n",
    "\n",
    "\n",
    "nn1_reloaded.eval() # sets dropout and batch normalisation layers to evaluation mode before inference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may also save and load entire models\n",
    "\n",
    "PATH = \"entire_model.pt\"\n",
    "\n",
    "#Save \n",
    "torch.save( nn1, PATH )\n",
    "\n",
    "#Load\n",
    "nn1_reloaded_whole = torch.load( PATH )"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
