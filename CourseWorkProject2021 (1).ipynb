{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5950J Course Project\n",
    "\n",
    "This project recaps and systematises work that has been done in the exercise notebooks: if you have worked through the notebooks, little extra work is required.  The aim of the project is to give you the experience of trying to develop a neural network classifier (or regressor), involving setting up a model, optimising the amount of regularisation, investigating its performance, and devising a new model. \n",
    "\n",
    "You should do your work in this notebook, filling in the sections below. To do the work, you may re-use code from ***any of the lab-session sheets provided so far***. (In fact, you should be able to do nearly the entire project using code taken from previous lab sessions.) \n",
    "\n",
    "Please complete this workbook and submit it on Moodle, with all outputs (numbers and graphs) visible and included. (I do not want to have to evaluate your notebook!) \n",
    "\n",
    "Ideally, do not re-use cells for multiple computations, accumulating the results by hand: have a a separate cell for each computation, so that I can look through the whole notebook and see what you have done, in sequence. This makes it much easier for me to understand what has happened if something has gone wrong.\n",
    "\n",
    "This project has **10%** of the marks of the course.\n",
    "\n",
    "The submission deadline is Wednesday 30th June at 4pm (on Moodle of course). \n",
    "\n",
    "Please write your student number **\n",
    "here**:     \n",
    "so that I have an identifier of which worksheet I am marking, to prevent any confusion! (I have to download your notebooks before marking them, so I don't want to accidentally give credit for your notebook to somebody else...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset\n",
    "\n",
    "You may choose any of the datasets included with Pytorch, or used in the course so far (including, for example, the percolation data). (It is a free country and you may use any other data you wish.)\n",
    "\n",
    "A safe choice would be either the MNIST data or the MNIST-fashion data, which is a drop-in replacement for MNIST (same size data format, same number of classes, same number of training and test examples). \n",
    "\n",
    "A more interesting choice would be CIFAR-10\n",
    "\n",
    "In setting up the data, you should set up a training set and a test set. The test set should be large enough to give a reasonably accurate assessment of the error-rate (or loss) of your models: preferably at least 10,000 examples.\n",
    "\n",
    "For the learning curve experiment (below), you will need to construct training sets of different sizes, with the largest at least 10 times the smallest. For the MNIST data, for example, your smallest training set might be 500, with sizes 500, 1000, 2000, 4000, 8000, 16000, 32000, and perhaps 60000 if you have time. (You get no extra marks for doing very long experiments.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model \n",
    "\n",
    "Set up a Pytorch model (you may find it helpful to keep it simple and fast to train). Train on an intermediate-sized dataset (if you are using MNIST, say 2000 or 4000). \n",
    "\n",
    "\n",
    "Plot the loss and error rate as a function of training epochs. \n",
    "\n",
    "\n",
    "###  1.  Assessment of initial model:    ***12 marks***\n",
    "\n",
    "Ensure that your model is complex enough to *overfit* the training data: that is the loss/error-rate on the training set should be below your target loss/error-rate, and the loss/error-rate on the validation set should be higher than this (preferably higher than your target error-rate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (you can freely used and modify code from course lab-sheets throughout this project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Regularisation experiment:  ***12 marks***\n",
    "\n",
    "Now select a type of regularisation (which may be L2, or dropout) and train your initial model with different amounts of regularisation. \n",
    "\n",
    "For L2 regularisation, compare 0 regularisation with very small amounts of regularisation -- try 0.0001, 0.001, 0.01, and see for what level of L2 regularisation there starts to be an effect. \n",
    "\n",
    "A small amount of regularisation may improve the validation set performance of your overfitted model; too much regularisation may make performance on the validation set worse. \n",
    "\n",
    "Train your model multiple times, applying different amounts of regularisation, and plot a graph showing the effect of different amounts of regularisation. \n",
    "\n",
    "Plot the amount of regularisation along the x axis, and the validation set performance (loss/error-rate) on the y axis.  How does regularisation affect the performance of your initial network, and what is the optimal amount? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Learning curve experiment:  ***12 marks***\n",
    "\n",
    "Now train your model (with the amount of regularisation you selected) on different amounts of training data. \n",
    "\n",
    "Plot the performance (loss/error-rate) of the model on the validation set against the size of the training set. \n",
    "This is typically best done on a log-log plot. \n",
    "\n",
    "Describe the approximate relationship between the training set size and loss / error-rate.  Does the network performance appear to improve as some power of the amount of data in the training set?  If so, by what power ? \n",
    "\n",
    "For example, a very good rate of improvement is for error-rate to be proportional to $\\dfrac{1}{\\sqrt{n}}$ where $n$ is the training set size.  For your model, the rate of improvement of validation error with training set size may not be as fast as this (or it may, who knows?)  The aim of this exercise is to find out what it actually is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Second model:   *** 14 marks ***\n",
    "\n",
    "Devise what you believe to be a better neural network architecture for the problem (e.g. for MNIST you might use a fully-connected network for your initial model, and then try a convolutional net for your second model).\n",
    "\n",
    "Repeat sections 2 (regularisation experiment), and determine whether it has better validation set performance than the first model, for an intermediate size of training set (e.g. 2000 or 4000 for the MNIST data). \n",
    "\n",
    "Repeat section 3. Does your model have a different learning curve from the first? Plot the learning curves for the first and second model on the same graph, to compare them. Comment: is there a more rapid reduction of error-rate with training set size for your second model?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
